## Welcome to Week 2

This week, we dive into the core of classical NLP by building our first text classification models. We'll learn to convert text into numbers and use them to make predictions.

1. **[Text Vectorization](./Text%20Vectorization):** We will explore _TF-IDF_ (Term Frequency-Inverse Document Frequency), a powerful method for representing text that moves beyond simple word counts.
2. **[Logistic Regression for Text Classification](./Logistic%20Regression%20for%20Text%20Classification):** Youâ€™ll learn to train and evaluate a classifier for tasks like sentiment analysis using metrics like Accuracy, Precision, Recall, and F1-score.
3. **[Word & Sentence Embeddings](./Word%20&%20Sentence%20Embeddings):** To conclude, we'll introduce modern representation techniques like _Word2Vec_, _GloVe_, and _Sentence-BERT_, understanding their advantages over traditional methods.
4. **[Assignments](./Assignments):** It's time to put our knowledge to the test. Let's tackle the assignments and practice our skills to reinforce our understanding.

>[!Note]
For submitting all asignments make a github repo and store the assignments in that repo.
Refer to the [**video**](https://www.youtube.com/watch?v=PQsJR8ci3J0) for making repo.
